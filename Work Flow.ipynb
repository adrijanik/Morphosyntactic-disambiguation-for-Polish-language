{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: POS Tagging\n",
    "Introduction\n",
    "\n",
    "There is an ongoing discussion whether the problem of part of speech tagging is already solved, at least for English (see Manning 2011), by reaching the tagging error rates similar or lower than the human inter-annotator agreement, which is ca. 97%. In the case of languages with rich morphology, such as Polish, there is however no doubt that the accuracies of around 91% delivered by taggers leave much to be desired and more work is needed to proclaim this task as solved.\n",
    "\n",
    "The aim of this proposed task is therefore to stimulate research in potentially new approaches to the problem of POS tagging of Polish, which will allow to close the gap between the tagging accuracy of systems available for English and languages with rich morphology.\n",
    "\n",
    "### Task definition\n",
    "Subtask (A): Morphosyntactic disambiguation and guessing\n",
    "\n",
    "Given a sequence of segments, each with a set of possible morphosyntactic interpretations, the goal of the task is to select the correct interpretation for each of the segments and provide an interpretation for segments for which only 'ign' interpretation has been given (segments unknown to the morphosyntactic dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL IMPORTS ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from itertools import islice, chain\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import time\n",
    "import progressbar\n",
    "from lxml import objectify, etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DATA PREPARATION ###\n",
    "def read_training_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a tar.gz file as a list of words\"\"\"\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "        pattern = '<\\?xml version=\"1\\.0\" encoding=\"UTF-8\"\\?>\\s*<\\!DOCTYPE cesAna SYSTEM \"xcesAnaIPI\\.dtd\">\\s*<cesAna xmlns\\:xlink=\"http\\:\\/\\/www\\.w3\\.org\\/1999\\/xlink\" version=\"1\\.0\" type=\"lex disamb\">\\s*<chunkList>\\s*(?P<chunks>[\\W\\s\\d\\w]+)<\\/chunkList>\\s*<\\/cesAna>'\n",
    "        chunks_block = re.search(pattern, content)\n",
    "        if chunks_block:\n",
    "            all_chunks = chunks_block.groups('chunks')\n",
    "            pattern = '<chunk type=\\\"s\\\">\\s*(?P<chunk>[.\\w\\W\\s]+?)<\\/chunk>\\s*'\n",
    "            chunks = re.findall(pattern, all_chunks[0])\n",
    "            return chunks\n",
    "        return None\n",
    "\n",
    "def create_dictionary_train(chunks):\n",
    "    print(\"Number of chunks: {0}\".format(len(chunks)))\n",
    "    words = {}\n",
    "    for chunk in chunks:\n",
    "        pattern = '(?P<token><tok>\\s*(?:[\\w\\W\\d.]+?)<\\/tok>\\s*?)(?:<ns\\/>)?'\n",
    "        tokens = re.findall(pattern, chunk)\n",
    "        for tok in tokens:\n",
    "            pattern = '<orth>(?P<orth>.+)<\\/orth>\\s*(?:[\\w\\W\\d.]+)'\n",
    "            orth = re.search(pattern, tok)\n",
    "            x = orth.group('orth')\n",
    "            pattern = '<lex><base>(?P<base>.+)<\\/base><ctag>(?P<ctag>.+)<\\/ctag><\\/lex>\\s*'\n",
    "            lexes = re.findall(pattern, tok)\n",
    "            words[x] = [lexes]\n",
    "    return words\n",
    "        \n",
    "    \n",
    "def create_dictionary_gold(chunks):\n",
    "    print(\"Number of chunks: {0}\".format(len(chunks)))\n",
    "    words = {}\n",
    "    for chunk in chunks:\n",
    "        pattern = '(?P<token><tok>\\s*(?:[\\w\\W\\d.]+?)<\\/tok>\\s*?)(?:<ns\\/>)?'\n",
    "        tokens = re.findall(pattern, chunk)\n",
    "        for tok in tokens:\n",
    "            pattern = '<orth>(?P<orth>.+)<\\/orth>\\s*(?:[\\w\\W\\d.]+)'\n",
    "            orth = re.search(pattern, tok)\n",
    "            x = orth.group('orth')\n",
    "            pattern = '<lex disamb=\\\"1\\\"><base>(?P<base>.+)<\\/base><ctag>(?P<ctag>.+)<\\/ctag><\\/lex>\\s*'\n",
    "            lexes = re.findall(pattern, tok)\n",
    "            words[x] = [lexes[0][1]]\n",
    "    return words\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% time chunks = read_training_data('train-gold.xml')\n",
    "% time words = create_dictionary_gold(chunks)\n",
    "print('Data size %d' % len(words))\n",
    "print(\"Unique words: {0}\".format(len(Counter(words))))\n",
    "words_sample = take(5, words)\n",
    "print(\"\\n\".join([\"{0} -> {1}\".format(x,words[x][0]) for x in words_sample]))\n",
    "chunks_train = read_training_data('train-analyzed.xml')\n",
    "words_train = create_dictionary_train(chunks_train)\n",
    "print('Data size %d' % len(words_train))\n",
    "reference_words = list(words_train.keys()) # words list in order\n",
    "chunks = pd.read_csv('pl-embeddings-skip_pure_words.txt', chunksize=1000000, delimiter=' ', header=None, encoding='utf-8')\n",
    "embeddings_df = pd.DataFrame()\n",
    "%time embeddings_df = pd.concat(chunk for chunk in chunks).sort_values(0)\n",
    "del embeddings_df[101]\n",
    "subset_of_embeddings = embeddings_df.loc[embeddings_df[0].isin(words_train.keys())]\n",
    "print(len(subset_of_embeddings))\n",
    "tmp = subset_of_embeddings\n",
    "subset_of_embeddings['interpretation'] =  [words[word][0] for word in tmp[0]]\n",
    "subset_of_embeddings['disamb'] = [False for i in range(len(subset_of_embeddings))]\n",
    "word_list_with_duplicates = []\n",
    "interpretation = []\n",
    "disamb = []\n",
    "def create_series_for_df():\n",
    "    i = 0\n",
    "    bar = progressbar.ProgressBar(max_value=progressbar.UnknownLength)\n",
    "    #print(words_train['A'])\n",
    "    print('A' in reference_words)\n",
    "    for word_train in reference_words:\n",
    "        try:\n",
    "            #print(words_train[word_train][0])\n",
    "            for k in words_train[word_train][0]: # iterate over interpretations\n",
    "                #print(k[1])\n",
    "                if words[word_train][0].strip() != k[1].strip(): \n",
    "                    disamb.append(0)\n",
    "                else:\n",
    "                    disamb.append(1)\n",
    "                word_list_with_duplicates.append(word_train)\n",
    "                interpretation.append(k[1])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "        bar.update(i)\n",
    "\n",
    "%time create_series_for_df()\n",
    "words_count = Counter(word_list_with_duplicates)\n",
    "\n",
    "subset_of_embeddings['Count'] = subset_of_embeddings[0].map(words_count)\n",
    "subset_of_embeddings.Count = subset_of_embeddings.Count.fillna(0).astype(int)\n",
    "subset_with_repetitions = pd.DataFrame(np.repeat(subset_of_embeddings.values, subset_of_embeddings['Count'].values, axis=0))\n",
    "data_tuples = list(zip(word_list_with_duplicates, interpretation, disamb))\n",
    "print(data_tuples[:10])\n",
    "sorted_repetitions_df = subset_with_repetitions.sort([0])\n",
    "data_tuples = sorted(data_tuples)\n",
    "test_df = pd.DataFrame([(i[1],i[2]) for i in data_tuples], )\n",
    "test_df.head()\n",
    "subset_with_repetitions['interpretation'] = test_df[0]\n",
    "subset_with_repetitions['disamb'] = test_df[1]\n",
    "print(\"One-hot representation of morphosynthactic forms\")\n",
    "result = pd.concat([subset_with_repetitions,pd.get_dummies(subset_with_repetitions['interpretation'])], axis=1)\n",
    "del result[101]\n",
    "del result[102]\n",
    "del result[103]\n",
    "del result['interpretation']\n",
    "tmp = result['disamb']\n",
    "del result['disamb']\n",
    "result = pd.concat([result, tmp],axis=1)\n",
    "result.head()\n",
    "with open('all_columns','w') as f:\n",
    "    f.write(str(result.columns.tolist()))\n",
    "    \n",
    "result.to_csv('input-output-dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SPLIT DATA INTO TRAINING AND TEST SETS ###\n",
    "correct = pd.DataFrame()\n",
    "non_correct = pd.DataFrame()\n",
    "correct_test = pd.DataFrame()\n",
    "non_correct_test = pd.DataFrame()\n",
    "\n",
    "bar_all = progressbar.ProgressBar(max_value=progressbar.UnknownLength)\n",
    "for j, chunk in enumerate(pd.read_csv('input-output-dataset.csv', chunksize=10000)):\n",
    "    del chunk['0']            \n",
    "    del chunk['Unnamed: 0']\n",
    "    \n",
    "    if j % 5 == 0:\n",
    "        correct_test = pd.concat([correct_test, chunk[chunk['disamb'] == True]])\n",
    "        non_correct_test = pd.concat([non_correct_test, chunk[chunk['disamb'] == False]])\n",
    "    else:\n",
    "        correct = pd.concat([correct, chunk[chunk['disamb'] == True]])\n",
    "        non_correct = pd.concat([non_correct, chunk[chunk['disamb'] == False]])\n",
    "        bar_all.update(j)\n",
    "\n",
    "del correct['disamb']\n",
    "del non_correct['disamb']\n",
    "del correct_test['disamb']\n",
    "del non_correct_test['disamb']\n",
    "\n",
    "with open('in-out_correct.csv', 'w') as f:\n",
    "    correct.to_csv(f, header=False, index=False)\n",
    "with open('in-out_non-correct.csv', 'w') as f:\n",
    "    non_correct.to_csv(f, header=False, index=False)\n",
    "with open('in-out_correct_test.csv', 'w') as f:\n",
    "    correct_test.to_csv(f, header=False, index=False)\n",
    "with open('in-out_non-correct_test.csv', 'w') as f:\n",
    "    non_correct_test.to_csv(f, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL CREATION AND TEACHING ###\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def add_missing_dummy_columns( d, columns ):\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0\n",
    "\n",
    "def fix_columns( d, columns ):\n",
    "\n",
    "    add_missing_dummy_columns( d, columns )\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert( set( columns ) - set( d.columns ) == set())\n",
    "\n",
    "    extra_cols = set( d.columns ) - set( columns )\n",
    "    for c in extra_cols:\n",
    "        if c not in list(map(lambda x: str(x),range(0,102))):\n",
    "            del d[c]\n",
    "        elif c not in d:\n",
    "            d[c] = 0\n",
    "\n",
    "def create_model():\n",
    "    tf.reset_default_graph()\n",
    "    accu = []\n",
    "    \n",
    "    # first layer\n",
    "    hid_layer = 140\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 1398])\n",
    "    y_train = tf.placeholder(tf.float32, shape=[None,2])\n",
    "\n",
    "    W_1 = weight_variable([1398, hid_layer])\n",
    "    b_1 = bias_variable([hid_layer])\n",
    "\n",
    "    h_1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W_1),b_1))\n",
    "\n",
    "    # second layer\n",
    "    W_2 = weight_variable([hid_layer, 2])\n",
    "    b_2 = bias_variable([2])\n",
    "\n",
    "    h_2 =  tf.nn.sigmoid(tf.add(tf.matmul(h_1,W_2),b_2))\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=y_train, logits=h_2))\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(h_2, 1), tf.argmax(y_train, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    chunksize = 10000\n",
    "    epochs = 1\n",
    "\n",
    "    bar = progressbar.ProgressBar(max_value=epochs)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for i in range(epochs):\n",
    "            print(\"Step: {0}\".format(i))\n",
    "            corr_chunks = pd.read_csv('in-out_correct.csv', chunksize=chunksize, delimiter=',')\n",
    "            non_corr_chunks = pd.read_csv('in-out_non-correct.csv', chunksize=chunksize, delimiter=',')\n",
    "\n",
    "            for chunk_corr, chunk_non_corr in zip(corr_chunks, non_corr_chunks):\n",
    "\n",
    "                batch_y = pd.concat([pd.DataFrame(np.ones(len(chunk_corr))), pd.DataFrame(np.zeros(len(chunk_non_corr)))]).values\n",
    "                batch_y_neg = pd.concat([pd.DataFrame(np.zeros(len(chunk_corr))), pd.DataFrame(np.ones(len(chunk_non_corr)))]).values\n",
    "                batch_y = np.hstack((batch_y, batch_y_neg))\n",
    "\n",
    "                batch_x = pd.concat([chunk_corr,chunk_non_corr], ignore_index=True)\n",
    "                batch_x = batch_x.values\n",
    "\n",
    "                #shuffle\n",
    "                combined = list(zip(batch_x, batch_y))\n",
    "                np.random.shuffle(combined)\n",
    "\n",
    "                batch_x[:], batch_y[:] = zip(*combined)\n",
    "\n",
    "                train_step.run(feed_dict={x: batch_x, y_train: batch_y})\n",
    "\n",
    "                print(\"Step accuracy.\")\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                    x: batch_x, y_train: batch_y})\n",
    "                print('step {0}, train accuracy {1:.10}'.format(i, train_accuracy))\n",
    "            bar.update(i)\n",
    "\n",
    "        test_corr = pd.read_csv('in-out_correct_test.csv',chunksize=chunksize, delimiter=',')\n",
    "        test_non_corr = pd.read_csv('in-out_non-correct_test.csv', chunksize=chunksize, delimiter=',')\n",
    "\n",
    "        output = []\n",
    "        for chunk_corr_tst, chunk_non_corr_tst in zip(test_corr, test_non_corr):\n",
    "            test_y = pd.concat([pd.DataFrame(np.ones(len(chunk_corr_tst))), pd.DataFrame(np.zeros(len(chunk_non_corr_tst)))]).values\n",
    "            test_y_neg = pd.concat([pd.DataFrame(np.zeros(len(chunk_corr_tst))), pd.DataFrame(np.ones(len(chunk_non_corr_tst)))]).values\n",
    "\n",
    "            test_y = np.hstack((test_y, test_y_neg))\n",
    "\n",
    "            tmp_1 = chunk_corr_tst.values\n",
    "            tmp_2 = chunk_non_corr_tst.values\n",
    "            test_x = np.vstack((tmp_1, tmp_2))\n",
    "            accu.append(accuracy.eval(feed_dict={x: test_x, y_train: test_y}))\n",
    "            print('test accuracy {0:.10f}'.format(accu[-1]))\n",
    "            output.append(h_2.eval(feed_dict={x: test_x}))\n",
    "\n",
    "        df = pd.DataFrame(output)\n",
    "        df.to_csv('output_test')\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(sess, \"./model.ckpt\")\n",
    "  `      print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TEST DATA PREPARATION ###\n",
    "def get_xcef_chunks(filename):\n",
    "    \"\"\"Function returns chunks from xcef file as a string\"\"\"\n",
    "    print(\"Reading chunks of data from file: {0}\".format(filename))\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "        pattern = '<\\?xml version=\"1\\.0\" encoding=\"UTF-8\"\\?>\\s*<\\!DOCTYPE cesAna SYSTEM \"xcesAnaIPI\\.dtd\">\\s*<cesAna xmlns\\:xlink=\"http\\:\\/\\/www\\.w3\\.org\\/1999\\/xlink\" version=\"1\\.0\" type=\"lex disamb\">\\s*<chunkList>\\s*(?P<chunks>[\\W\\s\\d\\w]+)<\\/chunkList>\\s*<\\/cesAna>'\n",
    "        chunks_block = re.search(pattern, content)\n",
    "        if chunks_block:\n",
    "            all_chunks = chunks_block.groups('chunks')\n",
    "            pattern = '<chunk type=\\\"s\\\">\\s*(?P<chunk>[.\\w\\W\\s]+?)<\\/chunk>\\s*'\n",
    "            chunks = re.findall(pattern, all_chunks[0])\n",
    "            return chunks\n",
    "        return None\n",
    "\n",
    "def create_dict_of_words(chunks):\n",
    "    \"\"\"Function creates dictionary with words as keys and gramatical interpretations as values\"\"\"\n",
    "    \n",
    "    print(\"Creating dictionary of words from chunks.\")\n",
    "    print(\"Number of chunks: {0}\".format(len(chunks)))\n",
    "    words = {}\n",
    "    i = 0 # index for each word\n",
    "    for chunk in chunks:\n",
    "        pattern = '(?P<token><tok>\\s*(?:[\\w\\W\\d.]+?)<\\/tok>\\s*?)(?:<ns\\/>)?'\n",
    "        tokens = re.findall(pattern, chunk) # get all tokens\n",
    "        for tok in tokens:\n",
    "            pattern = '<orth>(?P<orth>.+)<\\/orth>\\s*(?:[\\w\\W\\d.]+)'\n",
    "            orth = re.search(pattern, tok) \n",
    "            x = orth.group('orth') # get word\n",
    "            pattern = '<lex><base>(?:.+)<\\/base><ctag>(?P<ctag>.+)<\\/ctag><\\/lex>\\s*'\n",
    "            lexes = re.findall(pattern, tok) # get lexems\n",
    "            words[x] = ([lexes], i) # save for each word every possible lexem\n",
    "            i += 1\n",
    "    return words\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def prepare_training_data(file_test):\n",
    "\n",
    "    chunks = get_xcef_chunks(file_test)\n",
    "    words = create_dict_of_words(chunks)\n",
    "    first_5 = take(5, words)\n",
    "    print(\"First 5 words:\") \n",
    "    for i in first_5:\n",
    "        print(\"     {0}   -> {1}\".format(i, words[i]))\n",
    "    \n",
    "    print('Number of words: {0}'.format(len(words)))\n",
    "    print(\"Unique words: {0}\".format(len(Counter(words))))\n",
    "\n",
    "    reference_words = list(words.keys()) # words list in order\n",
    "\n",
    "    ### READ IN EMBEDDINGS ###\n",
    "    chunks = pd.read_csv('pl-embeddings-skip_pure_words.txt', chunksize=1000000, delimiter=' ', header=None, encoding='utf-8')\n",
    "    embeddings_df = pd.DataFrame()\n",
    "    embeddings_df = pd.concat(chunk for chunk in chunks).sort_values(0)\n",
    "    del embeddings_df[101]\n",
    "    #embeddings_df.head(30)\n",
    "\n",
    "    ### GET SUBSET OF EMBEDDINGS FOR ANALYZED DATA \n",
    "    subset_of_embeddings = embeddings_df.loc[embeddings_df[0].isin(words.keys())]\n",
    "    print(\"Subset of embeddings length: {0}\".format(len(subset_of_embeddings)))\n",
    "    tmp = subset_of_embeddings\n",
    "    subset_of_embeddings['interpretation'] =  [words[word][0][0] for word in tmp[0]]\n",
    "    subset_of_embeddings['index'] =  [words[word][1] for word in tmp[0]]\n",
    "    print(\"Subset of embeddings head: {0}\".format(subset_of_embeddings.head()))\n",
    "\n",
    "    word_list_with_duplicates = []\n",
    "    interpretation = []\n",
    "    index = []\n",
    "    def create_series_for_df():\n",
    "        print(\"Create series for df:\")\n",
    "        i = 0\n",
    "        for word_train in reference_words:\n",
    "            \n",
    "            #print(words[word_train][0][0])\n",
    "            for k in words[word_train][0][0]: # iterate over interpretations\n",
    "                #print(\"interp: {0}\".format(k))\n",
    "                word_list_with_duplicates.append(word_train)\n",
    "                interpretation.append(k)\n",
    "                #print(\"Index: {0}\".format(words[word_train][1]))\n",
    "                index.append(words[word_train][1])\n",
    "            i += 1\n",
    "\n",
    "    create_series_for_df()\n",
    "\n",
    "    ### PREPARE TRAINING DATA WITH EXTENSION OF OTHER INTERPRETATION ###\n",
    "    words_count = Counter(word_list_with_duplicates)\n",
    "    subset_of_embeddings['Count'] = subset_of_embeddings[0].map(words_count)\n",
    "    subset_of_embeddings.Count = subset_of_embeddings.Count.fillna(0).astype(int)\n",
    "\n",
    "    subset_with_repetitions = pd.DataFrame(np.repeat(subset_of_embeddings.values, subset_of_embeddings['Count'].values, axis=0))\n",
    "    print(\"Subset with repetitions head: {0}\".format(subset_with_repetitions.head()))\n",
    "    data_tuples = list(zip(word_list_with_duplicates, interpretation, index))\n",
    "    \n",
    "    sorted_repetitions_df = subset_with_repetitions.sort([102])\n",
    "    print(\"Subset with repetitions sorted by index head: {0}\".format(sorted_repetitions_df.head()))\n",
    "    data_tuples = sorted(data_tuples, key=lambda x: x[2])\n",
    "    print(\"Tupled data: {0}\".format(data_tuples[:15]))\n",
    "    test_df = pd.DataFrame([i[1], i[2]] for i in data_tuples )\n",
    "    #test_df.head()\n",
    "    subset_with_repetitions['interpretation'] = test_df[0]\n",
    "    subset_with_repetitions['index'] = test_df[1]\n",
    "\n",
    "#    subset_with_repetitions['disamb'] = test_df[1]\n",
    "    #subset_with_repetitions.tail(10)\n",
    "\n",
    "    print(\"One-hot representation of morphosynthactic forms\")\n",
    "    result = pd.concat([subset_with_repetitions,pd.get_dummies(subset_with_repetitions['interpretation'])], axis=1)\n",
    "    del result[101]\n",
    "    del result[102]\n",
    "    del result['interpretation']\n",
    "    print(result.head())\n",
    "    input_file = 'input_test.csv'\n",
    "    result.to_csv(input_file, encoding='utf-8')\n",
    "    print(\"Test data saved successfully in: {0}\".format(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL EVALUATION ON TEST DATA ###\n",
    "def write_tags_input_data(test_file, output_tags):\n",
    "    merged = list(itertools.chain(*output_tags))\n",
    "    out = pd.DataFrame(merged)\n",
    "    out.to_csv(\"output_tags.csv\")\n",
    "\n",
    "\n",
    "def check_model_unknown_output(test_file):\n",
    "    tf.reset_default_graph()\n",
    "    chunksize = 10000\n",
    "    test = pd.read_csv(test_file,chunksize=chunksize, delimiter=',')\n",
    "\n",
    "    # first layer\n",
    "    hid_layer = 140\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 1398])\n",
    "    y_train = tf.placeholder(tf.float32, shape=[None,2])\n",
    "\n",
    "    W_1 = weight_variable([1398, hid_layer])\n",
    "    b_1 = bias_variable([hid_layer])\n",
    "\n",
    "    h_1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W_1),b_1))\n",
    "\n",
    "    # second layer\n",
    "    W_2 = weight_variable([hid_layer, 2])\n",
    "    b_2 = bias_variable([2])\n",
    "\n",
    "    h_2 =  tf.nn.sigmoid(tf.add(tf.matmul(h_1,W_2),b_2))\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=y_train, logits=h_2))\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(h_2, 1), tf.argmax(y_train, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    chunksize = 10000\n",
    "    epochs = 2\n",
    "\n",
    "    bar = progressbar.ProgressBar(max_value=epochs)\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, \"./model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "        output = []\n",
    "        #prepare_test_data(test_file)\n",
    "        test = pd.read_csv('input_test.csv',chunksize=chunksize, delimiter=',')\n",
    "\n",
    "        for chunk in test:\n",
    "            del chunk['Unnamed: 0']\n",
    "            del chunk['0']\n",
    "            columns = None\n",
    "            with open('all_columns') as f:\n",
    "                columns = f.read().split(', ')\n",
    "\n",
    "            fix_columns( chunk, columns )\n",
    "            test_x = chunk.values\n",
    "            print(\"Size of test_x for chunk {0}\".format(len(test_x)))\n",
    "            test_out = h_2.eval(feed_dict={x: test_x})\n",
    "            print(\"Test output size: {0}\".format(len(test_out)))\n",
    "            output.append(test_out)\n",
    "\n",
    "        write_tags_input_data(test_file, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_model_unknown_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHECK IF SIZES ARE THE SAME ###\n",
    "out = pd.read_csv('output_tags.csv')\n",
    "input_tst = pd.read_csv('input_test.csv')\n",
    "assert ( len(out) == input_tst.shape[0])\n",
    "print(\"Length of output: {0}, shape of input data: {1}\".format(len(out),input_tst.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random():\n",
    "    return random.choice([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TAG INPUT FILE WITH CORRECT DISAMBIGUATIONS ###\n",
    "class Disambiguer():\n",
    "    def __init__(self, xml):\n",
    "        self.root = objectify.fromstring(xml)\n",
    "        self.output = pd.read_csv('output_tags.csv').values\n",
    "\n",
    "    def get_sentences(self):\n",
    "        self.sentences = []\n",
    "        self.chunks = []\n",
    "        for j in self.root.chunkList.chunk:\n",
    "            self.chunks.append(j.chunk)\n",
    "            for i in j.chunk.tok:\n",
    "                self.sentences.append(i.orth.text)\n",
    "        print(' '.join(self.sentences))\n",
    "\n",
    "        return self.chunks\n",
    "\n",
    "    def get_ctags(self):\n",
    "        self.ctags = []\n",
    "        for chunk in self.root.chunkList.chunk:\n",
    "            for i in chunk.tok:\n",
    "                for j in i.lex:\n",
    "                    self.ctags.append(j.ctag)\n",
    "        return self.ctags\n",
    "\n",
    "    def get_disambiguation(self, n, word, interpretation):\n",
    "        print(word + \"  \" + interpretation + \" \" + str(n))\n",
    "        if n >= len(self.output):\n",
    "            return get_random()\n",
    "        if float(self.output[n][0]) >= float(self.output[n][1]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_list_of_lexems(self):\n",
    "        lexems = []\n",
    "        ctags = self.get_ctags()\n",
    "        print(len(ctags))\n",
    "        for n, i in enumerate(ctags):\n",
    "            lexems.append(i.getparent())\n",
    "            tmp = i.getparent().getparent().orth\n",
    "            orth = tmp.text\n",
    "            disamb = self.get_disambiguation(n, orth, i.text)\n",
    "            if disamb == False:\n",
    "                i.getparent().getparent().remove(i.getparent())\n",
    "            else:\n",
    "                if len(i.getparent().getparent().lex) > 1:\n",
    "                    i.getparent().getparent().remove(i.getparent())\n",
    "                else:\n",
    "                    i.getparent().set('disamb', str(disamb))\n",
    "\n",
    "        out = etree.tostring(self.root, pretty_print=True, encoding='UTF-8', xml_declaration=False)\n",
    "        #out = out.encode('UTF-8')\n",
    "        with open('tagged_test_file.xml', 'wb') as f:\n",
    "            f.write(out)\n",
    "\n",
    "        return lexems\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def read_file(self):\n",
    "        with open(self.path, 'rb') as f:\n",
    "            content = f.read()\n",
    "            return content\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"./test-analyzed.xml\"\n",
    "data = DataReader(data_path)\n",
    "disamb = Disambiguer(data.read_file())\n",
    "#ctags = set(disamb.get_ctags())\n",
    "#print(\"Unique ctags: {0}\".format(len(ctags)))\n",
    "#print(sorted(list(map(lambda x: x.text,ctags))))\n",
    "disamb.get_list_of_lexems()\n",
    "#for i in disamb.get_list_of_lexems():\n",
    "#    print(i.attrib['disamb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
